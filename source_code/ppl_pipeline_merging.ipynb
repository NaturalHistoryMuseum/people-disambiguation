{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72f9e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "import time\n",
    "import re\n",
    "import recordlinkage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import ratio\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504fca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fuction is designed to find the id matched between wikidata and harvard index\n",
    "def define_true_pairs(indexList1, indexList2, indexName1, indexName2):\n",
    "    arrays = [indexList1, indexList2]\n",
    "    tuples = list(zip(*arrays))\n",
    "    index = pd.MultiIndex.from_tuples(tuples, names=[indexName1, indexName2])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2501caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# this function will remove all special charaters -- including spaces\n",
    "# but too slow comparing to replace -- used process_time() for evaluation\n",
    "def clean_text(text): # fa\n",
    "    a = \"\"\n",
    "    text = a.join(char for char in text if char.isalnum())\n",
    "    return text\n",
    "'''\n",
    "# Remove square blankets auto generated during data alignment process\n",
    "def clean_text(text): # fb\n",
    "    text = text.replace('[', '').replace(']','').replace(\"'\", '')\n",
    "    return text\n",
    "\n",
    "def remove_spec_in_col(df, col):\n",
    "    newCol = []\n",
    "    for index, rowValue in df[col].iteritems():\n",
    "        if pd.notnull(rowValue):\n",
    "            newCol.append(clean_text(rowValue))\n",
    "        else:\n",
    "            newCol.append(np.nan)\n",
    "    return newCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e58c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to split the label(full name) into first name and last name\n",
    "def split_full_name(full_name):\n",
    "    if pd.isnull(full_name) or full_name == '':\n",
    "        return '', ''\n",
    "    parts = full_name.split()\n",
    "    first_name = ' '.join(parts[:-1])\n",
    "    last_name = parts[-1]\n",
    "    return first_name, last_name\n",
    "\n",
    "# Define the function to convert each word in the first name to the desired format\n",
    "def convert_to_initial(name):\n",
    "    if pd.isnull(name) or name == '':\n",
    "        return ''\n",
    "    initials = [word[0].upper() + '.' for word in name.split()]\n",
    "    return ' '.join(initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "534dadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to combine two multi index objects\n",
    "def combine_multi_index(index1, index2):\n",
    "    # Convert MultiIndex objects to DataFrames\n",
    "    multi_index1 = index1.to_frame(index=False)\n",
    "    multi_index2 = index2.to_frame(index=False)\n",
    "\n",
    "    # Concatenate the DataFrames\n",
    "    combined_index = pd.concat([multi_index1, multi_index2])\n",
    "\n",
    "    # Remove duplicates\n",
    "    combined_index = combined_index.drop_duplicates()\n",
    "\n",
    "    # Convert back to MultiIndex\n",
    "    combined_multi_index = pd.MultiIndex.from_frame(combined_index)\n",
    "    \n",
    "    return combined_multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_initial(name):\n",
    "    parts = name.split()\n",
    "    return all(len(part) == 2 and part[0].isalpha() and part[1] == '.' for part in parts)\n",
    "\n",
    "def names_not_completely_different(name1, name2, threshold):\n",
    "    return ratio(name1, name2) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to find the possible matching records between two datasets\n",
    "def generate_match_pairs(dataset1, dataset2, lastName_threshold, firstName_threshold):  \n",
    "\n",
    "    # Initialize indexer\n",
    "    indexer = recordlinkage.Index()\n",
    "\n",
    "    # Sorted neighborhood indexing on lastname\n",
    "    indexer.sortedneighbourhood('lastName', window=3)\n",
    "    \n",
    "    # Blocking on author abbreviation - generate index pairs if 'exact match on author abbreviation'\n",
    "    indexer.block(left_on = 'authorAbbrv', right_on='B & P Author Abbrev.')\n",
    "\n",
    "    candidate_links = indexer.index(dataset1, dataset2)\n",
    "\n",
    "    compare_cl = recordlinkage.Compare()\n",
    "\n",
    "    # Exact match on author abbreviation, extra conditioning on the blocking\n",
    "    compare_cl.exact('authorAbbrv', 'B & P Author Abbrev.', label='authorAbbrv')\n",
    "    \n",
    "    # Compare last names with Levenshtein distance\n",
    "    # When threshold set to none, the feature table stores the similarity percentage calculated using Levenshtein distance\n",
    "    compare_cl.string('lastName', 'lastName', method='damerau_levenshtein', threshold=None, label='lastName')\n",
    "\n",
    "    # Compare first names with Levenshtein distance\n",
    "    compare_cl.string('firstName', 'firstName', method='damerau_levenshtein', threshold=None, label='firstName')\n",
    "    \n",
    "    # Exact match on first name initial\n",
    "    compare_cl.exact('firstName_initial', 'firstName_initial',label='firstName_initial')\n",
    "\n",
    "    # Exact match on first name initial from label(normalised full name)\n",
    "    compare_cl.exact('first_name_initial', 'first_name_initial', label='first_name_initial')\n",
    "\n",
    "    # Exact match on date of birth\n",
    "    compare_cl.exact('dateOfBirth', 'birthYear', label='dateOfBirth')\n",
    "\n",
    "    # Exact match on date of death\n",
    "    compare_cl.exact('dateOfDeath', 'deathYear', label='dateOfDeath')\n",
    "\n",
    "    # Compute the comparison results\n",
    "    features = compare_cl.compute(candidate_links, dataset1, dataset2)\n",
    "    # print(len(features))\n",
    "    \n",
    "    # Filter pairs based on the given criteria\n",
    "    index_pairs = []\n",
    "    # Set the threshold of names_not_completely_different function (firstname check after initial comparison)\n",
    "    threshold = 0.5\n",
    "\n",
    "    for index, row in features.iterrows():\n",
    "        a_index = index[0]\n",
    "        b_index = index[1]\n",
    "        \n",
    "        # Store the record pairs if their author abbreviations are the same\n",
    "        if row['authorAbbrv'] == 1:\n",
    "            index_pairs.append(index)\n",
    "            \n",
    "        # Waterfall model to check criteria\n",
    "        if row['lastName'] >= lastName_threshold:\n",
    "            if row['firstName'] >= firstName_threshold:\n",
    "                index_pairs.append(index)\n",
    "            if row['firstName_initial'] == 1:\n",
    "                firstName1 = dataset1.at[index[0], 'firstName']\n",
    "                firstName2 = dataset2.at[index[1], 'firstName']\n",
    "                if not (is_initial(firstName1) or is_initial(firstName2)) and names_not_completely_different(firstName1, firstName2, threshold):\n",
    "                    index_pairs.append(index)\n",
    "            if row['first_name_initial'] == 1:\n",
    "                if not (is_initial(firstName1) or is_initial(firstName2)) and names_not_completely_different(firstName1, firstName2, threshold):\n",
    "                    index_pairs.append(index)\n",
    "            if row['firstName_initial'] == 1 and row['dateOfBirth'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            if row['first_name_initial'] == 1 and row['dateOfBirth'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            # if row['dateOfBirth'] == 1:\n",
    "            #     index_pairs.append(index)\n",
    "            if row['dateOfBirth'] == 1 and row['dateOfDeath'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            # Check if b's label is in a's aliases list\n",
    "            if pd.notna(dataset1.at[a_index, 'aliases']) and row['dateOfBirth'] == 1:\n",
    "                aliases_list = [alias.strip() for alias in dataset1.at[a_index, 'aliases'].split(',')]\n",
    "                for alias in aliases_list:\n",
    "                    if pd.notna(dataset2.at[b_index, 'Standard/Label Name']) and pd.notna(alias) and ratio(dataset2.at[b_index, 'Standard/Label Name'], alias) >= 0.9:\n",
    "                        index_pairs.append(index)\n",
    "                        break              \n",
    "            # Check if a's label is in b's aliases list\n",
    "            if pd.notna(dataset2.at[b_index, 'Name']) and row['dateOfBirth'] == 1:\n",
    "                aliases_list = [alias.strip() for alias in dataset2.at[b_index, 'Name'].split(',')]\n",
    "                for alias in aliases_list:\n",
    "                    if pd.notna(dataset1.at[a_index, 'label']) and pd.notna(alias) and ratio(dataset1.at[a_index, 'label'], alias) >= 0.9:\n",
    "                        index_pairs.append(index)\n",
    "                        break\n",
    "\n",
    "    # Remove duplicate pairs\n",
    "    index_pairs = list(set(index_pairs))\n",
    "    \n",
    "    # Convert index_pairs to a MultiIndex object\n",
    "    multi_index = pd.MultiIndex.from_tuples(index_pairs, names=[\"wikiID\", \"harvardIndex\"])\n",
    "\n",
    "    return multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to find the possible matching records between two datasets\n",
    "def generate_match_pairs_bio(dataset1, dataset2, lastName_threshold, firstName_threshold):  \n",
    "\n",
    "    # Initialize indexer\n",
    "    indexer = recordlinkage.Index()\n",
    "\n",
    "    # Sorted neighborhood indexing on lastname\n",
    "    indexer.sortedneighbourhood('lastName', window=3)\n",
    "    \n",
    "    # Blocking on author abbreviation - generate index pairs if 'exact match on author abbreviation'\n",
    "    # indexer.block(left_on = 'authorAbbrv', right_on='B & P Author Abbrev.')\n",
    "\n",
    "    candidate_links = indexer.index(dataset1, dataset2)\n",
    "    # print(len(candidate_links))\n",
    "\n",
    "    compare_cl = recordlinkage.Compare()\n",
    "\n",
    "    # Exact match on author abbreviation, extra conditioning on the blocking\n",
    "    # compare_cl.exact('authorAbbrv', 'B & P Author Abbrev.', label='authorAbbrv')\n",
    "    \n",
    "    # Compare last names with Levenshtein distance\n",
    "    # When threshold set to none, the feature table stores the similarity percentage calculated using Levenshtein distance\n",
    "    compare_cl.string('lastName', 'lastName', method='damerau_levenshtein', threshold=None, label='lastName')\n",
    "\n",
    "    # Compare first names with Levenshtein distance\n",
    "    compare_cl.string('firstName', 'firstName', method='damerau_levenshtein', threshold=None, label='firstName')\n",
    "    \n",
    "    # Exact match on first name initial\n",
    "    compare_cl.exact('firstName_initial', 'firstName_initial',label='firstName_initial')\n",
    "\n",
    "    # Exact match on first name initial from label(normalised full name)\n",
    "    compare_cl.exact('first_name_initial', 'first_name_initial', label='first_name_initial')\n",
    "\n",
    "    # Exact match on date of birth\n",
    "    compare_cl.exact('dateOfBirth', 'dateOfBirth', label='dateOfBirth')\n",
    "\n",
    "    # Exact match on date of death\n",
    "    compare_cl.exact('dateOfDeath', 'dateOfDeath', label='dateOfDeath')\n",
    "\n",
    "    # Compute the comparison results\n",
    "    features = compare_cl.compute(candidate_links, dataset1, dataset2)\n",
    "    # print(len(features))\n",
    "    \n",
    "    # Filter pairs based on the given criteria\n",
    "    index_pairs = []\n",
    "    \n",
    "    # Set the threshold of names_not_completely_different function (firstname check after initial comparison)\n",
    "    threshold = 0.7\n",
    "\n",
    "    for index, row in features.iterrows():\n",
    "        a_index = index[0]\n",
    "        c_index = index[1]\n",
    "        \n",
    "        # Bionomia data does not have author abbreviation column to be matched\n",
    "        # Store the record pairs if their author abbreviations are the same\n",
    "        # if row['authorAbbrv'] == 1:\n",
    "        #     index_pairs.append(index)\n",
    "            \n",
    "        # Waterfall model to check criteria\n",
    "        if row['lastName'] >= lastName_threshold:\n",
    "            if row['firstName'] >= firstName_threshold:\n",
    "                index_pairs.append(index)\n",
    "            if row['firstName_initial'] == 1:\n",
    "                firstName1 = dataset1.at[index[0], 'firstName']\n",
    "                firstName2 = dataset2.at[index[1], 'firstName']\n",
    "                if not (is_initial(firstName1) or is_initial(firstName2)) and names_not_completely_different(firstName1, firstName2, threshold):\n",
    "                    index_pairs.append(index)\n",
    "            if row['first_name_initial'] == 1:\n",
    "                if not (is_initial(firstName1) or is_initial(firstName2)) and names_not_completely_different(firstName1, firstName2, threshold):\n",
    "                    index_pairs.append(index)\n",
    "            if row['firstName_initial'] == 1 and row['dateOfBirth'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            if row['first_name_initial'] == 1 and row['dateOfBirth'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            # if row['dateOfBirth'] == 1:\n",
    "            #     index_pairs.append(index)\n",
    "            if row['dateOfBirth'] == 1 and row['dateOfDeath'] == 1:\n",
    "                index_pairs.append(index)\n",
    "            # Check if b's label is in a's aliases list\n",
    "            if pd.notna(dataset1.at[a_index, 'aliases']) and row['dateOfBirth'] == 1:\n",
    "                aliases_list = [alias.strip() for alias in dataset1.at[a_index, 'aliases'].split(',')]\n",
    "                for alias in aliases_list:\n",
    "                    if pd.notna(dataset2.at[c_index, 'label']) and pd.notna(alias) and ratio(dataset2.at[c_index, 'label'], alias) >= 0.9:\n",
    "                        index_pairs.append(index)\n",
    "                        break              \n",
    "            # Check if a's label is in b's aliases list\n",
    "            if pd.notna(dataset2.at[c_index, 'acceptedNames']) and row['dateOfBirth'] == 1:\n",
    "                aliases_list = [alias.strip() for alias in dataset2.at[c_index, 'acceptedNames'].split(',')]\n",
    "                for alias in aliases_list:\n",
    "                    if pd.notna(dataset1.at[a_index, 'label']) and pd.notna(alias) and ratio(dataset1.at[a_index, 'label'], alias) >= 0.9:\n",
    "                        index_pairs.append(index)\n",
    "                        break\n",
    "\n",
    "    # Remove duplicate pairs\n",
    "    index_pairs = list(set(index_pairs))\n",
    "    \n",
    "    # Convert index_pairs to a MultiIndex object\n",
    "    multi_index = pd.MultiIndex.from_tuples(index_pairs, names=[\"wikiID\", \"bioID\"])\n",
    "\n",
    "    return multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "a = pd.read_csv('a.csv',chunksize=10000,encoding='utf-8',on_bad_lines='skip',engine='python')\n",
    "a = pd.concat(a)\n",
    "a['wikiID'] = a['id']\n",
    "a = a.set_index('id')\n",
    "\n",
    "b = pd.read_csv('b.csv',chunksize=10000,encoding='utf-8',on_bad_lines='skip',engine='python')\n",
    "b = pd.concat(b)\n",
    "b['harvardIndex'] = b['id']\n",
    "b = b.set_index('id')\n",
    "\n",
    "c = pd.read_csv('c.csv',chunksize=10000,encoding='utf-8',on_bad_lines='skip',engine='python')\n",
    "c = pd.concat(c)\n",
    "c['acceptedNames'] = remove_spec_in_col(c,'acceptedNames')\n",
    "c.rename(columns={'Unnamed: 0': 'bioID'}, inplace=True)\n",
    "\n",
    "# Find out the ground truth matches of wikidata and harvard index using id in the records\n",
    "a['harvardIndex'] = pd.to_numeric(a['harvardIndex'],errors='coerce') \n",
    "temp = pd.merge(a, b, how='inner', on=None, left_on='harvardIndex', right_on='harvardIndex',\n",
    "                  left_index=False, right_index=False, sort=False,\n",
    "                  suffixes=('_wiki', '_harvard'), copy=False, indicator=False)\n",
    "\n",
    "true_matches_WH = define_true_pairs(temp['wikiID'],temp['harvardIndex'].astype(int),'wikiID','harvardIndex')\n",
    "# print(\"\\nWikiID and havardIndex pairs as true matches:\")\n",
    "# print(true_matches)\n",
    "\n",
    "# Print out the precentage\n",
    "print('There are '+ str(len(true_matches_WH)) +' HarvardIndex records in Wikidata, which is ' + str(len(true_matches_WH)/len(a)*100) +'%')\n",
    "print('There are '+ str(len(true_matches_WH)) +' Wikidata records in HarvardIndex, which is ' + str(len(true_matches_WH)/len(b)*100) +'%')\n",
    "\n",
    "# Find out the ground truth matches of wikidata and bionomia using id in the records\n",
    "temp = pd.merge(a, c, how='inner', on=None, left_on='wikiID', right_on='wikidata',\n",
    "                  left_index=False, right_index=False, sort=False,\n",
    "                  suffixes=('_wiki', '_bionomia'), copy=False, indicator=False)\n",
    "\n",
    "true_matches_WB = define_true_pairs(temp['wikiID'],temp['bioID'].astype(int),'wikiID','bioID')\n",
    "# print(\"\\nWikiID and bionomia pairs as true matches:\")\n",
    "# print(true_matches)\n",
    "\n",
    "# Print out the precentage\n",
    "print('There are '+ str(len(true_matches_WB)) +' Bionomia records in Wikidata, which is ' + str(len(true_matches_WB)/len(a)*100) +'%')\n",
    "print('There are '+ str(len(true_matches_WB)) +' Wikidata records in Bionomia, which is ' + str(len(true_matches_WB)/len(c)*100) +'%')\n",
    "\n",
    "\n",
    "# Preprocess and clean data\n",
    "dfa = a.copy()\n",
    "dfb = b.copy()\n",
    "dfc = c.copy()\n",
    "\n",
    "# Set indices\n",
    "dfa.set_index('wikiID', inplace=True)\n",
    "dfb.set_index('harvardIndex', inplace=True)\n",
    "dfc.set_index('bioID', inplace=True)\n",
    "\n",
    "# Apply the split_full_name function to separate first name and last name\n",
    "dfa[['first_name', 'last_name']] = dfa['label'].apply(lambda x: pd.Series(split_full_name(x)))\n",
    "# Apply the convert_to_initial function to the first name column\n",
    "dfa['first_name_initial'] = dfa['first_name'].apply(convert_to_initial)\n",
    "\n",
    "# Apply the split_full_name function to separate first name and last name\n",
    "dfb[['first_name', 'last_name']] = dfb['Standard/Label Name'].apply(lambda x: pd.Series(split_full_name(x)))\n",
    "# Apply the convert_to_initial function to the first name column\n",
    "dfb['first_name_initial'] = dfb['first_name'].apply(convert_to_initial)\n",
    "\n",
    "# Apply the split_full_name function to separate first name and last name\n",
    "dfc[['first_name', 'last_name']] = dfc['label'].apply(lambda x: pd.Series(split_full_name(x)))\n",
    "# Apply the convert_to_initial function to the first name column\n",
    "dfc['first_name_initial'] = dfc['first_name'].apply(convert_to_initial)\n",
    "\n",
    "# Apply the convert_to_initial function to the first name column in Wiki\n",
    "dfa['firstName_initial'] = dfa['firstName'].apply(convert_to_initial)\n",
    "# Apply the convert_to_initial function to the first name column in HI\n",
    "dfb['firstName_initial'] = dfb['firstName'].apply(convert_to_initial)\n",
    "# Apply the convert_to_initial function to the first name column in Bionomia\n",
    "dfc['firstName_initial'] = dfc['firstName'].apply(convert_to_initial)\n",
    "\n",
    "# Drop duplicated last name column\n",
    "dfa.drop('last_name', axis=1, inplace=True)\n",
    "dfb.drop('last_name', axis=1, inplace=True)\n",
    "dfc.drop('last_name', axis=1, inplace=True)\n",
    "\n",
    "print('Finished data preparation for Wikidata, HI data and Bionomia data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all possible matches between wikidata and harvard index (all data)\n",
    "temp_index = generate_match_pairs(dfa, dfb, lastName_threshold=0.9, firstName_threshold=0.85)\n",
    "# print(len(temp_index))\n",
    "\n",
    "# Combine the possible matches with the true matches generated by exact id matching\n",
    "index_pairs_WH = combine_multi_index(temp_index, true_matches_WH)\n",
    "print(\"There are {} possible matches found based on the given criteria\".format(len(index_pairs_WH)))\n",
    "\n",
    "# Print out the precentage\n",
    "print('There are '+ str(len(index_pairs_WH)) +' HarvardIndex records in Wikidata, which is ' + str(len(index_pairs_WH)/len(a)*100) +'%')\n",
    "print('There are '+ str(len(index_pairs_WH)) +' Wikidata records in HarvardIndex, which is ' + str(len(index_pairs_WH)/len(b)*100) +'%')\n",
    "\n",
    "# Merge the matches with the original data\n",
    "matched_dfa = dfa.loc[index_pairs_WH.get_level_values('wikiID')].reset_index()\n",
    "matched_dfb = dfb.loc[index_pairs_WH.get_level_values('harvardIndex')].reset_index()\n",
    "\n",
    "# Combine the matched DataFrames side by side\n",
    "combined_matches_WH = pd.concat([matched_dfa, matched_dfb], axis=1)\n",
    "\n",
    "# Optionally add a label to identify matched rows\n",
    "combined_matches_WH['matched'] = True\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_matches_WH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all possible matches between wikidata and harvard index (all data)\n",
    "temp_index = generate_match_pairs_bio(dfa, dfc, lastName_threshold=0.9, firstName_threshold=0.85)\n",
    "# print(len(temp_index))\n",
    "\n",
    "# Combine the possible matches with the true matches generated by exact id matching\n",
    "index_pairs_WB = combine_multi_index(temp_index, true_matches)\n",
    "print(\"There are {} possible matches found based on the given criteria\".format(len(index_pairs_WB)))\n",
    "\n",
    "# Print out the precentage\n",
    "print('There are '+ str(len(index_pairs_WB)) +' Bionomia records in Wikidata, which is ' + str(len(index_pairs_WB)/len(a)*100) +'%')\n",
    "print('There are '+ str(len(index_pairs_WB)) +' Wikidata records in Bionomia, which is ' + str(len(index_pairs_WB)/len(c)*100) +'%')\n",
    "\n",
    "# Merge the matches with the original data\n",
    "matched_dfa = dfa.loc[index_pairs_WB.get_level_values('wikiID')].reset_index()\n",
    "matched_dfc = dfc.loc[index_pairs_WB.get_level_values('bioID')].reset_index()\n",
    "\n",
    "# Combine the matched DataFrames side by side\n",
    "combined_matches_WB = pd.concat([matched_dfa, matched_dfc], axis=1)\n",
    "\n",
    "# Optionally add a label to identify matched rows\n",
    "combined_matches_WB['matched'] = True\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_matches_WB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
